{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    roc_curve, roc_auc_score, auc,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_recall_curve, f1_score, average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# models simples\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# models ensemble\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carreguem la base de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. EDA (exploratory data analysis) (1 punt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check basic info about the data set including missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "t=train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "d = train.describe()\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comptar NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "nan_count = train.isnull().sum()\n",
    "nan_percentage = train.isnull().mean() * 100\n",
    "print(\"Número de NaNs por columna:\")\n",
    "print(nan_count)\n",
    "print(\"\\nPorcentaje de NaNs por columna:\")\n",
    "print(nan_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory analysis and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot a bar diagram to check the number of numeric entries**\n",
    "\n",
    "From the bar diagram, it shows that there are some age entries missing as the number of count for 'Age' is less than the other counts. We can do some impute/transformation of the data to fill-up the missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "dT=d.T\n",
    "dT.plot.bar(y='count')\n",
    "plt.title(\"Bar plot of the count of numeric features\",fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the relative size of survived and not-survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Survived', data=train, palette='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there a pattern for the survivability based on sex?**\n",
    "\n",
    "It looks like more female survived than males!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following code extracts and plots the fraction of passenger count that survived, by each class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train['Survived'] = train['Survived'].astype(float)\n",
    "f_class_survived = train.groupby('Pclass')['Survived'].mean()\n",
    "f_class_survived = pd.DataFrame(f_class_survived)\n",
    "f_class_survived.plot.bar(y='Survived')\n",
    "plt.title(\"Fraction of passengers survived by class\",fontsize=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What about any pattern related to having sibling and spouse?**\n",
    "\n",
    "It looks like there is a weak trend that chance of survibility increased if there were more number of sibling or spouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Survived',hue='SibSp',data=train,palette='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "f_class_Age=train.groupby('Pclass')['Age'].mean()\n",
    "f_class_Age = pd.DataFrame(f_class_Age)\n",
    "f_class_Age.plot.bar(y='Age')\n",
    "plt.title(\"Average age of passengers by class\",fontsize=17)\n",
    "plt.ylabel(\"Age (years)\", fontsize=17)\n",
    "plt.xlabel(\"Passenger class\", fontsize=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing (normalitzation, outlier removal, feature selection..) (2 punts)\n",
    "* Escala de les dades.\n",
    "* Impute age (by averaging) - Tractament de NaNs.\n",
    "* Drop unncessary features\n",
    "* Convert categorical features to dummy variables - Encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Crear el imputador, usando 'median' para rellenar los valores faltantes\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "train['Age'] = imputer.fit_transform(train['Age'].values.reshape(-1, 1))\n",
    "\n",
    "# Describir el dataframe y hacer un gráfico de barras\n",
    "d = train.describe()\n",
    "dT = d.T\n",
    "dT.plot.bar(y='count')\n",
    "plt.title(\"Bar plot of the count of numeric features\",fontsize=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exctracting extra features\n",
    "\n",
    "Before start dropping NaN features I found that we could take some extra features, where some of them are based of features that are about to be droppend due to their high NaN count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# hasCabin is a binary feature that says if the passanger had a Cabin or not\n",
    "train['hasCabin'] = train['Cabin'].notna()\n",
    "\n",
    "# hasFamiliar is a binary feature that says if the passanger had a familiar or not (from SibSp)\n",
    "train['hasFamiliar'] = train['SibSp'] != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary features like 'PassengerId', 'Name', 'Ticket', 'Cabin' and any other null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train.drop(['PassengerId','Name','Ticket', 'Cabin'],axis=1,inplace=True)\n",
    "train.dropna(inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorial feature like 'Sex' and 'Embarked' to dummy variables and then drop the 'Sex' and 'Embarked' columns and concatenate the new dummy variables\n",
    "\n",
    "**Use pandas 'get_dummies()' function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sex = pd.get_dummies(train['Sex'])\n",
    "embark = pd.get_dummies(train['Embarked'])\n",
    "train.drop(['Sex','Embarked'],axis=1,inplace=True)\n",
    "train = pd.concat([train,sex,embark],axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escala de dades.\n",
    "\n",
    "Primer mirem com queden les nostres dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train.plot(kind='box')\n",
    "plt.title('Comparació de l\\'escala dels atributs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara normalitzem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "X = train.drop('Survived', axis=1).to_numpy()  # Característiques\n",
    "y = train['Survived'].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "num_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "\n",
    "train[num_cols].plot(kind='box')\n",
    "plt.title('Comparación de la escala de los atributos después de la normalización')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Polinomial features\n",
    "\n",
    "As extra features we can add extra features which can help de model find any quadratic, cubic or basically polynomic correlation between certain features and the label, in this case we find usefull to use Numbered features such as Age and Fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_polynomial_features(data, columns, degree):\n",
    "    # Copy the DataFrame\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "\n",
    "    for c in columns:\n",
    "        # Select the specific column to apply polynomial features\n",
    "        selected_data = data_copy[[c]]\n",
    "        poly_data = poly.fit_transform(selected_data)\n",
    "\n",
    "        # Create column names for the new polynomial features\n",
    "        poly_column_names = [f\"{c}_poly_{i}\" for i in range(poly_data.shape[1])]\n",
    "        print(poly_column_names)\n",
    "\n",
    "        # Add the polynomial features to the DataFrame\n",
    "        data_copy[poly_column_names] = poly_data\n",
    "\n",
    "    # Drop the original columns that were transformed\n",
    "    data_copy = data_copy.drop(columns=columns)\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "train_sq = create_polynomial_features(train, ['Age', 'Fare'], 2)\n",
    "train_sq.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Preprocessing Feature Selection\n",
    "\n",
    "##### Finding multicolineality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = train.corr()\n",
    "\n",
    "# Trobar correlacions altes\n",
    "highly_correlated = (correlation_matrix > 0.8) & (correlation_matrix < 1.0)\n",
    "print(highly_correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Reducing high dimensions:\n",
    "\n",
    "To detect high dimensions, we calculate the correlation matrix between the variables, computed in the previous cell. Variables with low or close-to-zero correlations can be considered high dimensions because they contain little relevant information. One of the indicators of the height of dimensions is the eigenspace of the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Càlcul dels autovectors i autovectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "\n",
    "# Trobar les dimensions amb baixes contribucions\n",
    "explained_variance_ratio = eigenvalues / sum(eigenvalues)\n",
    "print([[var, x] for var, x in zip(explained_variance_ratio, train.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, as we can see, we obtain low correlation in attributes except for Pclass, Age, and SibSp. We will create a new dataframe with these variables that have a higher correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "cutted_train = train[['Pclass', 'Age', 'SibSp']]\n",
    "\n",
    "X = train.drop('Survived', axis=1)  # Característiques\n",
    "y = train['Survived']\n",
    "\n",
    "X_train_cutted, X_test_cutted, y_train_cutted, y_test_cutted = train_test_split(X , y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_cutted = scaler.fit_transform(X_train_cutted)\n",
    "X_test_cutted = scaler.transform(X_test_cutted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Aplicant PCA en tots les datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_PCA(x, y, components=2, title='2 component PCA'):\n",
    "    pca = PCA(n_components=components)\n",
    "    \n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "    ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "    ax.set_title(title, fontsize = 20)\n",
    "    \n",
    "    ax.scatter(principalComponents[::10,0], principalComponents[::10,1],c = y[::10], s = 40, cmap='viridis')\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "plot_PCA(train.drop(columns=['Survived']), train['Survived'], title='PCA amb train')\n",
    "plot_PCA(train_sq.drop(columns=['Survived']), train['Survived'], title='PCA amb train_sq')\n",
    "plot_PCA(cutted_train, train['Survived'], title='PCA amb cutted_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see, with PCA, we obtain principal components with fairly scattered points in the normal training dataset.\n",
    "\n",
    "Meanwhile, with PCA squared, we no longer obtain normalized data because we have quadratic attributes that exaggerate the values we previously normalized, creating extreme cases like the last two that are around 60000.\n",
    "\n",
    "Finally, with the dataset trimmed to those with higher linear correlation, we obtain principal components that even seem to have some correlation, although they are still scattered between one class and another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Metric selection (1.5 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model de proba\n",
    "\n",
    "We'll use a super simple logistic regression model to have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para mostrar el performance del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def show_performance(x, y, model, title='confusion matrix', average='binary', figsize=(5, 5)):\n",
    "    predictions = model.predict(x)\n",
    "    acc = accuracy_score(y, predictions)\n",
    "    prec = precision_score(y, predictions, average=average)\n",
    "    rec = recall_score(y, predictions, average=average)\n",
    "    f1 = f1_score(y, predictions, average=average)\n",
    "\n",
    "    conf_mat = confusion_matrix(y, predictions)\n",
    "    conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print(f'Accuracy:{acc}')\n",
    "    print(f'Precision:{prec}')\n",
    "    print(f'Recall:{rec}')\n",
    "    print(f'F1-score:{f1}')\n",
    "    \n",
    "    disp=ConfusionMatrixDisplay(conf_mat)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para mostrar la curva ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def generate_roc(X_test, y_test, model):\n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [1 for _ in range(len(y_test))]\n",
    "    # predict probabilities\n",
    "    lr_probs = model.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "\n",
    "    ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "    # summarize scores\n",
    "    print('Classificador sense capacitat predictiva: ROC AUC=%.3f' % (ns_auc))\n",
    "    print('El nostre model: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "    lr_fpr, lr_tpr, lr_threshold = roc_curve(y_test, lr_probs)\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Sense capacitat predictiva')\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label='Nostre')\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para mostrar la curva Precision/Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def generate_PR(X_test, y_test, model):\n",
    "     # generate a no skill prediction (majority class)\n",
    "    ns_probs = [1 for _ in range(len(y_test))]\n",
    "    # predict probabilities\n",
    "    lr_probs = model.predict_proba(X_test)\n",
    "    \n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    # summarize score\n",
    "    print('El nostre model té una auc=%.3f' % (lr_auc))\n",
    "    # plot the precision-recall curves\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='Sense capacitat predictiva')\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Nostre')\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "X_train_MS, X_test_MS, y_train_MS, y_test_MS = train_test_split(X_train , y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_MS, y_train_MS)\n",
    "\n",
    "generate_roc(X_test_MS, y_test_MS, model)\n",
    "generate_PR(X_test_MS, y_test_MS, model)\n",
    "\n",
    "show_performance(X_test_MS, y_test_MS, model)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_MS = model.predict(X_test_MS)\n",
    "\n",
    "# Gener_MSate a classification report\n",
    "report = classification_report(y_test_MS, y_pred_MS)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Selection amb validacio creuada (4 punts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creem StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "def grid_search_param(model, param_grid, X_train, y_train):\n",
    "    grid = GridSearchCV(model, param_grid, cv=stratified_kfold, scoring='f1')\n",
    "    grid.fit(X_train, y_train)\n",
    "    grid.cv_results_.keys()\n",
    "    \n",
    "    # Dictionary containing the parameters used to generate that score\n",
    "    print(f'Best parameters: {grid.best_params_}')\n",
    "    # Single best score achieved across all params (k)\n",
    "    print(f'Best F1 found: {round(grid.best_score_, 5)}')\n",
    "    return grid.best_estimator_\n",
    "\n",
    "def cv_scores(model, X_train, y_train):\n",
    "    start_time = time.time()\n",
    "    score = cross_val_score(model, X_train, y_train, cv=stratified_kfold, scoring='f1')\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"f1 score: \", score.mean())\n",
    "    print(\"Time for CV: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "param_grid = dict(n_neighbors=list(range(5, 20)), p=[1,2], weights=['uniform', 'distance'] )\n",
    "print(f'Grid: {param_grid}')\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "best_knn = grid_search_param(knn, param_grid, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resultats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def_knn = KNeighborsClassifier()\n",
    "print(\"Default scores:\")\n",
    "cv_scores(def_knn, X_train, y_train)\n",
    "print(\"Best estimator scores:\")\n",
    "cv_scores(best_knn, X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "param_grid = dict(C=[0.01, 0.1, 1, 10, 100],\n",
    "                    tol=[1e-2, 1e-3, 1e-4], \n",
    "                    class_weight=[None, 'balanced'])\n",
    "print(f'Grid: {param_grid}')\n",
    "\n",
    "svc = SVC(random_state=0, kernel='linear', probability=True)\n",
    "best_svc = grid_search_param(svc, param_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resultats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def_svc = SVC()\n",
    "print(\"Default scores:\")\n",
    "cv_scores(def_svc, X_train, y_train)\n",
    "print(\"Best estimator scores:\")\n",
    "cv_scores(best_svc, X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "param_grid = dict(n_estimators=list(range(10, 51, 5)), \n",
    "                  learning_rate=[0.01, 0.05, 0.1, 0.5, 1],\n",
    "                  base_estimator__tol=[1e-3, 1e-4], \n",
    "                  base_estimator__class_weight=[None, 'balanced'])\n",
    "print(f'Grid: {param_grid}')\n",
    "\n",
    "ada_boost = AdaBoostClassifier(SVC(random_state=0, kernel='linear', probability=True))\n",
    "best_ada_svc = grid_search_param(ada_boost, param_grid, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resultats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def_ada = AdaBoostClassifier()\n",
    "print(\"Default scores:\")\n",
    "cv_scores(def_ada, X_train, y_train)\n",
    "print(\"Best estimator scores:\")\n",
    "cv_scores(best_ada_svc, X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "param_grid = dict(n_estimators=list(range(75, 151, 25)), \n",
    "                  max_depth=list(range(4, 8, 1)), \n",
    "                  criterion=['gini', 'entropy'],\n",
    "                  max_features=['sqrt', 'log2'])\n",
    "print(f'Grid: {param_grid}')\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "best_rf = grid_search_param(rf, param_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resultats:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def_rf = RandomForestClassifier()\n",
    "print(\"Default scores:\")\n",
    "cv_scores(def_rf, X_train, y_train)\n",
    "print(\"Best estimator scores:\")\n",
    "cv_scores(best_rf, X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Analisi Final (1.5 punt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#knn\n",
    "show_performance(X_test, y_test, best_knn)\n",
    "generate_roc(X_test, y_test, best_knn)\n",
    "\n",
    "#svc\n",
    "show_performance(X_test, y_test, best_svc)\n",
    "generate_roc(X_test, y_test, best_knn)\n",
    "\n",
    "#rf\n",
    "show_performance(X_test, y_test, best_rf)\n",
    "generate_roc(X_test, y_test, best_rf)\n",
    "\n",
    "#adaboost\n",
    "show_performance(X_test, y_test, best_rf)\n",
    "generate_roc(X_test, y_test, best_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
